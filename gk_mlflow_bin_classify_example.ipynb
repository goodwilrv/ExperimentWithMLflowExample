{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align='left'>  MLFlow Example - Gautam Kumar - Data Scientist II, Ex-AWS, Vancouver </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([2975,  525]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here I am Creating an skewed binary classification dataset\n",
    "X, y = make_classification(n_samples=3500, n_features=10, n_informative=2, n_redundant=8, \n",
    "                           weights=[0.85, 0.15], flip_y=0, random_state=21)\n",
    "\n",
    "np.unique(y, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_traing ------->>>>>>>>. X_traing.shape   (2625, 10)\n",
      "Shape of Y_traing ------->>>>>>>>. Y_traing.shape   (2625,)\n",
      "Shape of X_test ------->>>>>>>>. X_test.shape   (875, 10)\n",
      "Shape of Y_test ------->>>>>>>>. Y_test.shape   (875,)\n"
     ]
    }
   ],
   "source": [
    "# Below code splits the the dataset to training and testing data\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=21)\n",
    "\n",
    "print(\"Shape of X_traing ------->>>>>>>>. X_traing.shape  \", X_train.shape)\n",
    "print(\"Shape of Y_traing ------->>>>>>>>. Y_traing.shape  \", y_train.shape)\n",
    "print(\"Shape of X_test ------->>>>>>>>. X_test.shape  \", X_test.shape)\n",
    "print(\"Shape of Y_test ------->>>>>>>>. Y_test.shape  \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp 1: Classification by training Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GK Classification report for Logistic Regression:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.97      0.95       744\n",
      "           1       0.76      0.63      0.69       131\n",
      "\n",
      "    accuracy                           0.92       875\n",
      "   macro avg       0.85      0.80      0.82       875\n",
      "weighted avg       0.91      0.92      0.91       875\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logistic_regr = LogisticRegression(C=1, solver='liblinear')\n",
    "logistic_regr.fit(x_train, y_train)\n",
    "y_pred_log_reg = logistic_regr.predict(x_test)\n",
    "print(\"GK Classification report for Logistic Regression:\" + classification_report(y_test, y_pred_log_reg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp 2: Classification by training training Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GK Classification report for Random Forest:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97       744\n",
      "           1       0.91      0.76      0.83       131\n",
      "\n",
      "    accuracy                           0.95       875\n",
      "   macro avg       0.93      0.87      0.90       875\n",
      "weighted avg       0.95      0.95      0.95       875\n",
      "\n"
     ]
    }
   ],
   "source": [
    "randomFrest_classifier = RandomForestClassifier(n_estimators=30, max_depth=3)\n",
    "randomFrest_classifier.fit(x_train, y_train)\n",
    "y_pred_ranFor = randomFrest_classifier.predict(x_test)\n",
    "print(\"GK Classification report for Random Forest:\" + classification_report(y_test, y_pred_ranFor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp 3: Classification by Training xgBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.97       744\n",
      "           1       0.83      0.77      0.80       131\n",
      "\n",
      "    accuracy                           0.94       875\n",
      "   macro avg       0.90      0.87      0.88       875\n",
      "weighted avg       0.94      0.94      0.94       875\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_classifier = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "xgb_classifier.fit(x_train, y_train)\n",
    "y_pred_xgb = xgb_classifier.predict(x_test)\n",
    "print(classification_report(y_test, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imblearn in /Users/user/opt/anaconda3/lib/python3.8/site-packages (0.0)\n",
      "Requirement already satisfied: imbalanced-learn in /Users/user/opt/anaconda3/lib/python3.8/site-packages (from imblearn) (0.12.3)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /Users/user/opt/anaconda3/lib/python3.8/site-packages (from imbalanced-learn->imblearn) (1.3.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Users/user/opt/anaconda3/lib/python3.8/site-packages (from imbalanced-learn->imblearn) (1.5.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/user/opt/anaconda3/lib/python3.8/site-packages (from imbalanced-learn->imblearn) (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/user/opt/anaconda3/lib/python3.8/site-packages (from imbalanced-learn->imblearn) (1.22.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/user/opt/anaconda3/lib/python3.8/site-packages (from imbalanced-learn->imblearn) (2.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp 4: Manage skewed classes using SMOTE and then training XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([2152, 2152]))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "smote = SMOTETomek(random_state=21)\n",
    "x_train_res, y_train_res = smote.fit_resample(x_train, y_train)\n",
    "\n",
    "np.unique(y_train_res, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GK Classification report for smote and xgboost:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.94      0.95       744\n",
      "           1       0.70      0.84      0.76       131\n",
      "\n",
      "    accuracy                           0.92       875\n",
      "   macro avg       0.83      0.89      0.86       875\n",
      "weighted avg       0.93      0.92      0.92       875\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_classifier = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "xgb_classifier.fit(x_train_res, y_train_res)\n",
    "\n",
    "## Predict the text observations\n",
    "y_pred_xgb = xgb_classifier.predict(x_test)\n",
    "print(\"GK Classification report for smote and xgboost:\" + classification_report(y_test, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_traing ------->>>>>>>>. X_traing.shape   (2625, 10)\n",
      "Shape of Y_traing ------->>>>>>>>. Y_traing.shape   (2625,)\n",
      "Shape of X_test ------->>>>>>>>. X_test.shape   (875, 10)\n",
      "Shape of Y_test ------->>>>>>>>. Y_test.shape   (875,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Shape of X_traing ------->>>>>>>>. X_traing.shape  \", X_train.shape)\n",
    "print(\"Shape of Y_traing ------->>>>>>>>. Y_traing.shape  \", y_train.shape)\n",
    "print(\"Shape of X_test ------->>>>>>>>. X_test.shape  \", X_test.shape)\n",
    "print(\"Shape of Y_test ------->>>>>>>>. Y_test.shape  \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"Left\" style=\"color:Red\">Below code Tracks the experiments using mlflow</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dict = [\n",
    "    (\n",
    "        \"Logistic Regression\", \n",
    "        LogisticRegression(C=1, solver='liblinear'), \n",
    "        (x_train, y_train),\n",
    "        (x_test, y_test)\n",
    "    ),\n",
    "    (\n",
    "        \"Random Forest\", \n",
    "        RandomForestClassifier(n_estimators=30, max_depth=3), \n",
    "        (x_train, y_train),\n",
    "        (x_test, y_test)\n",
    "    ),\n",
    "    (\n",
    "        \"XGBClassifier\",\n",
    "        XGBClassifier(use_label_encoder=False, eval_metric='logloss'), \n",
    "        (x_train, y_train),\n",
    "        (x_test, y_test)\n",
    "    ),\n",
    "    (\n",
    "        \"XGBClassifier With SMOTE\",\n",
    "        XGBClassifier(use_label_encoder=False, eval_metric='logloss'), \n",
    "        (x_train_res, y_train_res),\n",
    "        (x_test, y_test)\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting the model ------->>>>>>>>. this_model_name   Logistic Regression\n",
      "fitting the model ------->>>>>>>>. this_model_name   Random Forest\n",
      "fitting the model ------->>>>>>>>. this_model_name   XGBClassifier\n",
      "fitting the model ------->>>>>>>>. this_model_name   XGBClassifier With SMOTE\n"
     ]
    }
   ],
   "source": [
    "reports = []\n",
    "\n",
    "for this_model_name, this_model, train_set, test_set in models_dict:\n",
    "    X_train = train_set[0]\n",
    "    y_train = train_set[1]\n",
    "    X_test = test_set[0]\n",
    "    y_test = test_set[1]\n",
    "    \n",
    "    print(\"fitting the model ------->>>>>>>>. this_model_name  \", this_model_name)\n",
    "\n",
    "    \n",
    "    this_model.fit(X_train, y_train)\n",
    "    y_pred = this_model.predict(X_test)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    reports.append(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import mlflow.xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/21 16:00:55 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n",
      "2024/08/21 16:00:55 INFO mlflow.tracking._tracking_service.client: 🏃 View run Logistic Regression at: http://127.0.0.1:5000/#/experiments/843273881458092239/runs/2cd8ec5769d94748b2437472d14d46f8.\n",
      "2024/08/21 16:00:55 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/843273881458092239.\n",
      "2024/08/21 16:00:58 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n",
      "2024/08/21 16:00:58 INFO mlflow.tracking._tracking_service.client: 🏃 View run Random Forest at: http://127.0.0.1:5000/#/experiments/843273881458092239/runs/a2b0af618480433aa7b8d7b8cc6dee92.\n",
      "2024/08/21 16:00:58 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/843273881458092239.\n",
      "2024/08/21 16:00:58 INFO mlflow.tracking._tracking_service.client: 🏃 View run XGBClassifier at: http://127.0.0.1:5000/#/experiments/843273881458092239/runs/2b446c0f740f4990a019535f2cd2f1c8.\n",
      "2024/08/21 16:00:58 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/843273881458092239.\n"
     ]
    },
    {
     "ename": "NotFittedError",
     "evalue": "need to call fit or load_model beforehand",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-a69530316932>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"XGB\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxgboost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/mlflow/xgboost/__init__.py\u001b[0m in \u001b[0;36mlog_model\u001b[0;34m(xgb_model, artifact_path, conda_env, code_paths, registered_model_name, signature, input_example, await_registration_for, pip_requirements, extra_pip_requirements, model_format, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0mmetadata\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlogged\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \"\"\"\n\u001b[0;32m--> 264\u001b[0;31m     return Model.log(\n\u001b[0m\u001b[1;32m    265\u001b[0m         \u001b[0martifact_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0martifact_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mflavor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxgboost\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/mlflow/models/model.py\u001b[0m in \u001b[0;36mlog\u001b[0;34m(cls, artifact_path, flavor, registered_model_name, await_registration_for, metadata, run_id, resources, **kwargs)\u001b[0m\n\u001b[1;32m    696\u001b[0m                 \u001b[0martifact_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0martifact_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresources\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresources\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m             )\n\u001b[0;32m--> 698\u001b[0;31m             \u001b[0mflavor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlflow_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmlflow_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    699\u001b[0m             \u001b[0;31m# `save_model` calls `load_model` to infer the model requirements, which may result in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m             \u001b[0;31m# __pycache__ directories being created in the model directory.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/mlflow/xgboost/__init__.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(xgb_model, path, conda_env, code_paths, mlflow_model, signature, input_example, pip_requirements, extra_pip_requirements, model_format, metadata)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;31m# Save an XGBoost model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m     \u001b[0mxgb_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_data_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m     \u001b[0mxgb_model_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_fully_qualified_class_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     pyfunc.add_to_model(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(self, fname)\u001b[0m\n\u001b[1;32m    903\u001b[0m         \u001b[0mmeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_estimator_type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m         \u001b[0mmeta_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 905\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_booster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscikit_learn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmeta_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    906\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_booster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_booster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscikit_learn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mget_booster\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    803\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 805\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"need to call fit or load_model beforehand\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    806\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Booster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFittedError\u001b[0m: need to call fit or load_model beforehand"
     ]
    }
   ],
   "source": [
    "# Initialize MLflow\n",
    "mlflow.set_experiment(\"Anomaly Detection\")\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "\n",
    "for i, element in enumerate(models):\n",
    "    model_name = element[0]\n",
    "    model = element[1]\n",
    "    report = reports[i]\n",
    "    \n",
    "    with mlflow.start_run(run_name=model_name):        \n",
    "        mlflow.log_param(\"model\", model_name)\n",
    "        mlflow.log_metric('accuracy', report['accuracy'])\n",
    "        mlflow.log_metric('recall_class_1', report['1']['recall'])\n",
    "        mlflow.log_metric('recall_class_0', report['0']['recall'])\n",
    "        mlflow.log_metric('f1_score_macro', report['macro avg']['f1-score'])        \n",
    "        \n",
    "        if \"XGB\" in model_name:\n",
    "            mlflow.xgboost.log_model(model, \"model\")\n",
    "        else:\n",
    "            mlflow.sklearn.log_model(model, \"model\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
